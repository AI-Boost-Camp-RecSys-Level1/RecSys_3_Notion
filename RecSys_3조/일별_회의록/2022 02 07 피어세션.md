# 2022/02/07 피어세션

Author: 정훈 박, IJ K, 지누, JB S, Jin Sang woo
Date: February 7, 2022
작성자: IJ K

아침 스크럼: 딥러닝 기본까지 다 듣고 얘기해보기 (질문 3개씩 들고오기)

## Peer Session

MLP 실습 코드 관련 질의응답

- line by line

데이터 사이언스 인터뷰

[https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/](https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/)

- Gradient Descent에 대해서 쉽게 설명한다면?
    
    답) 최소점을 찾아야 되는 문제에서 각 점에 대한 기울기를 계산해서 기울기가 줄어드는 방향으로 계속해서 움직여 최소점을 찾아내는 알고리즘
    
    답?) 주어진 데이터를 가장 잘 설명할 수 있는 모델의 파라미터 조합을 찾는 방법 중 하나로, loss가 최소가 되도록 하기 위해 각 점에 대한 기울기를 계산해서 기울기가 줄어드는 방향으로 계속 움직여 최소점을 찾는 알고리즘
    
    - 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?
        
        답) 주어진 데이터를 제일 잘 표현하는 모델(의 파라미터 조합)을 찾고 싶다고 하자. 이때 실제 데이터 값과 예측 값의 차이를 loss function으로 나타낼 수 있고, 따라서 적절한 모델을 찾는 문제는 이 loss function의 최소점을 찾고자 하는 문제로 간단화 할 수 있다. 이를 설명하는 space 내에서 가로축은 각 parameter (e.g. weight, bias)라고 할 수 있고, 세로축은 loss function 값이라 할 수 있다. loss function이 convex하다 가정할 때, 이 function위의 현재 지점에서 local minimum으로 가는 최단방향은 gradient을 통해 알 수 있다. 하지만 실제 상황에서의 search space는 2차원이 아닌 다차원이므로 더욱이 convex함을 보장할 수 없다. 
        
    - GD 중에 때때로 Loss가 증가하는 이유는?
        
        $$
        w ← w-a\frac{\partial \,Loss}{\partial w}
        $$
        
        답) learning rate(보폭)는 hyper-parameter이므로  loss function space 내 각 지점에서 gradient descent에 의해 움직일 때 optimization point를 지나쳐버릴 수도 있다.
        
        - hyper-parameter? 모델링 할 때 사용자가 직접 세팅해주는 값으로, 여러 번의 시행착오를 통해 적절한 값을 설정해야 한다.
    - 중학생이 이해할 수 있게 더 쉽게 설명 한다면?
        
        답) 공을 높은 곳에서 굴릴 때 낮은 경사를 향해서 계속해서 굴러가는 것이 gradient descent라고 볼 수 있다. 원하는 것은 제일 낮은 point (global minimum)를 향해서 가는 것이지만, 가속도(learning rate) 때문에 그 점을 지나칠 수도 있다는 것은 위의 문제에 대한 답 (GD중에 때때로 Loss가 증가하는 이유)이고, 초기 지점을 적절하게 설정하지 못해 local minimum에 갇히는 경우도 있다. 
        
    - Back Propagation에 대해서 쉽게 설명 한다면?
        
        답) back propagation같은 경우 feed forward상황에서 사용한 weight와 같은 parameter들을 loss에서 미분한 값으로, gradient descent가 작동하는 원리와 같이 원하는 값 label으로 가는 방향성에 대한 정보를 얻을 수 있다.